{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as standard_transforms\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "\n",
    "from data_loader import Rescale\n",
    "from data_loader import RescaleT\n",
    "from data_loader import RandomCrop\n",
    "from data_loader import ToTensor\n",
    "from data_loader import ToTensorLab\n",
    "from data_loader import SalObjDataset\n",
    "\n",
    "from u2net import U2NET\n",
    "\n",
    "import time\n",
    "import datetime\n",
    "import gdown\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "bce_loss = nn.BCELoss(size_average=True)\n",
    "\n",
    "\n",
    "def muti_bce_loss_fusion(d0, d1, d2, d3, d4, d5, d6, labels_v):\n",
    "    loss0 = bce_loss(d0, labels_v)\n",
    "    loss1 = bce_loss(d1, labels_v)\n",
    "    loss2 = bce_loss(d2, labels_v)\n",
    "    loss3 = bce_loss(d3, labels_v)\n",
    "    loss4 = bce_loss(d4, labels_v)\n",
    "    loss5 = bce_loss(d5, labels_v)\n",
    "    loss6 = bce_loss(d6, labels_v)\n",
    "\n",
    "    loss = loss0 + loss1 + loss2 + loss3 + loss4 + loss5 + loss6\n",
    "    print(\"l0: %3f, l1: %3f, l2: %3f, l3: %3f, l4: %3f, l5: %3f, l6: %3f\\n\" % (\n",
    "        loss0.data.item(), loss1.data.item(), loss2.data.item(), loss3.data.item(), loss4.data.item(),\n",
    "        loss5.data.item(),\n",
    "        loss6.data.item()))\n",
    "\n",
    "    return loss0, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model_name = f'u2net_{datetime.datetime.now().date()}'\n",
    "\n",
    "images_path='textseg/image/'\n",
    "masks_path='textseg/semantic_label/'\n",
    "\n",
    "model_dir = os.path.join(os.getcwd(), 'saved_models', model_name + os.sep)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "model_name = f'u2net_{datetime.datetime.now().date()}'\n",
    "\n",
    "data_dir = os.path.join(os.getcwd(), 'train_data' + os.sep)\n",
    "tra_image_dir = os.path.join('DUTS', 'DUTS-TR', 'DUTS-TR', 'im_aug' + os.sep)\n",
    "tra_label_dir = os.path.join('DUTS', 'DUTS-TR', 'DUTS-TR', 'gt_aug' + os.sep)\n",
    "\n",
    "image_ext = '.jpg'\n",
    "label_ext = '.png'\n",
    "\n",
    "\n",
    "\n",
    "epoch_num = 100000\n",
    "batch_size_train = 12\n",
    "batch_size_val = 1\n",
    "train_num = 0\n",
    "val_num = 0\n",
    "\n",
    "tra_img_name_list = glob.glob(data_dir + tra_image_dir + '*' + image_ext)\n",
    "\n",
    "tra_lbl_name_list = []\n",
    "for img_path in tra_img_name_list:\n",
    "    img_name = img_path.split(os.sep)[-1]\n",
    "\n",
    "    aaa = img_name.split(\".\")\n",
    "    bbb = aaa[0:-1]\n",
    "    imidx = bbb[0]\n",
    "    for i in range(1, len(bbb)):\n",
    "        imidx = imidx + \".\" + bbb[i]\n",
    "\n",
    "    tra_lbl_name_list.append(data_dir + tra_label_dir + imidx + label_ext)\n",
    "\n",
    "print(\"---\")\n",
    "print(\"train images: \", len(tra_img_name_list))\n",
    "print(\"train labels: \", len(tra_lbl_name_list))\n",
    "print(\"---\")\n",
    "\n",
    "train_num = len(tra_img_name_list)\n",
    "\n",
    "salobj_dataset = SalObjDataset(\n",
    "    img_name_list=tra_img_name_list,\n",
    "    lbl_name_list=tra_lbl_name_list,\n",
    "    transform=transforms.Compose([\n",
    "        RescaleT(320),\n",
    "        RandomCrop(288),\n",
    "        ToTensorLab(flag=0)]))\n",
    "salobj_dataloader = DataLoader(salobj_dataset, batch_size=batch_size_train, shuffle=True, num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "net = U2NET(3, 1)\n",
    "if torch.cuda.is_available():\n",
    "    net.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(\"---define optimizer...\")\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(\"---start training...\")\n",
    "ite_num = 0\n",
    "running_loss = 0.0\n",
    "running_tar_loss = 0.0\n",
    "ite_num4val = 0\n",
    "save_frq = 2000  # save the model every 2000 iterations\n",
    "\n",
    "for epoch in range(0, epoch_num):\n",
    "    net.train()\n",
    "\n",
    "    for i, data in enumerate(salobj_dataloader):\n",
    "        ite_num = ite_num + 1\n",
    "        ite_num4val = ite_num4val + 1\n",
    "\n",
    "        inputs, labels = data['image'], data['label']\n",
    "\n",
    "        inputs = inputs.type(torch.FloatTensor)\n",
    "        labels = labels.type(torch.FloatTensor)\n",
    "\n",
    "        # wrap them in Variable\n",
    "        if torch.cuda.is_available():\n",
    "            inputs_v, labels_v = Variable(inputs.cuda(), requires_grad=False), Variable(labels.cuda(),\n",
    "                                                                                        requires_grad=False)\n",
    "        else:\n",
    "            inputs_v, labels_v = Variable(inputs, requires_grad=False), Variable(labels, requires_grad=False)\n",
    "\n",
    "        # y zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        d0, d1, d2, d3, d4, d5, d6 = net(inputs_v)\n",
    "        loss2, loss = muti_bce_loss_fusion(d0, d1, d2, d3, d4, d5, d6, labels_v)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # # print statistics\n",
    "        running_loss += loss.data.item()\n",
    "        running_tar_loss += loss2.data.item()\n",
    "\n",
    "        # del temporary outputs and loss\n",
    "        del d0, d1, d2, d3, d4, d5, d6, loss2, loss\n",
    "\n",
    "        print(\"[epoch: %3d/%3d, batch: %5d/%5d, ite: %d] train loss: %3f, tar: %3f \" % (\n",
    "            epoch + 1, epoch_num, (i + 1) * batch_size_train, train_num, ite_num, running_loss / ite_num4val,\n",
    "            running_tar_loss / ite_num4val))\n",
    "\n",
    "        if ite_num % save_frq == 0:\n",
    "            torch.save(net.state_dict(), model_dir + model_name + \"_bce_itr_%d_train_%3f_tar_%3f.pth\" % (\n",
    "                ite_num, running_loss / ite_num4val, running_tar_loss / ite_num4val))\n",
    "            running_loss = 0.0\n",
    "            running_tar_loss = 0.0\n",
    "            net.train()  # resume train\n",
    "            ite_num4val = 0\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}