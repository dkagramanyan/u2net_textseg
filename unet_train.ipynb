{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Conv2D, BatchNormalization, MaxPooling2D, concatenate, UpSampling2D, Dropout\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import unet\n",
    "from skimage import io, transform\n",
    "import time\n",
    "import skimage\n",
    "from matplotlib import pyplot as plt\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class TextSegDataset(tf.keras.utils.Sequence):\n",
    "\n",
    "    def __init__(self, images_paths, masks_paths, output_size, validation_split, batch_size=1, type='train',seed=968):\n",
    "\n",
    "        self.type = type\n",
    "        self.batch_size = batch_size\n",
    "        self.output_size = output_size\n",
    "        self.validation_split = validation_split\n",
    "        self.images_paths_train, self.images_paths_test, self.masks_paths_train,self.masks_paths_test = train_test_split(\n",
    "            images_paths, masks_paths, test_size=self.validation_split,random_state=seed)\n",
    "\n",
    "    def __len__(self):\n",
    "        l = None\n",
    "        if self.type == 'train':\n",
    "            l = len(self.images_paths_train)//self.batch_size\n",
    "        elif self.type == 'test':\n",
    "            l = len(self.images_paths_test)//self.batch_size\n",
    "        return l\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        batch_x = None\n",
    "        batch_y = None\n",
    "\n",
    "        if self.type == 'train':\n",
    "            batch_x = self.images_paths_train[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "            batch_y = self.masks_paths_train[idx * self.batch_size:(idx + 1) *\n",
    "                                                                   self.batch_size]\n",
    "        elif self.type == 'test':\n",
    "            batch_x = self.images_paths_test[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "            batch_y = self.masks_paths_test[idx * self.batch_size:(idx + 1) *\n",
    "                                                                  self.batch_size]\n",
    "        output_x = []\n",
    "        for file_name in batch_x:\n",
    "            img = transform.resize(io.imread(file_name)/255, (self.output_size, self.output_size))\n",
    "            output_x.append(img)\n",
    "\n",
    "        output_y = []\n",
    "        for file_name in batch_y:\n",
    "            img = transform.resize(io.imread(file_name)/255, (self.output_size, self.output_size,1))\n",
    "            output_y.append(img)\n",
    "\n",
    "        return (np.array(output_x),np.array(output_y))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class Unet(tf.keras.Model):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Unet, self).__init__()\n",
    "\n",
    "    def __call__(self, inputs):\n",
    "        conv1 = Conv2D(64, 3, activation='relu', dilation_rate=2, padding='same', kernel_initializer='he_normal')(\n",
    "            inputs)\n",
    "        conv1 = BatchNormalization()(conv1)\n",
    "        conv1 = Conv2D(64, 3, activation='relu', dilation_rate=2, padding='same', kernel_initializer='he_normal')(conv1)\n",
    "        conv1 = BatchNormalization()(conv1)\n",
    "        pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "        conv2 = Conv2D(128, 3, activation='relu', dilation_rate=2, padding='same', kernel_initializer='he_normal')(\n",
    "            pool1)\n",
    "        conv2 = BatchNormalization()(conv2)\n",
    "        conv2 = Conv2D(128, 3, activation='relu', dilation_rate=2, padding='same', kernel_initializer='he_normal')(\n",
    "            conv2)\n",
    "        conv2 = BatchNormalization()(conv2)\n",
    "        pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "        conv3 = Conv2D(256, 3, activation='relu', padding='same', kernel_initializer='he_normal')(pool2)\n",
    "        conv3 = BatchNormalization()(conv3)\n",
    "        conv3 = Conv2D(256, 3, activation='relu', padding='same', kernel_initializer='he_normal')(conv3)\n",
    "        conv3 = BatchNormalization()(conv3)\n",
    "        pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
    "        conv4 = Conv2D(512, 3, activation='relu', padding='same', kernel_initializer='he_normal')(pool3)\n",
    "        conv4 = BatchNormalization()(conv4)\n",
    "        conv4 = Conv2D(512, 3, activation='relu', padding='same', kernel_initializer='he_normal')(conv4)\n",
    "        conv4 = BatchNormalization()(conv4)\n",
    "        drop4 = Dropout(0.5)(conv4, training=True)\n",
    "        pool4 = MaxPooling2D(pool_size=(2, 2))(drop4)\n",
    "\n",
    "        conv5 = Conv2D(1024, 3, activation='relu', padding='same', kernel_initializer='he_normal')(pool4)\n",
    "        conv5 = BatchNormalization()(conv5)\n",
    "        conv5 = Conv2D(1024, 3, activation='relu', padding='same', kernel_initializer='he_normal')(conv5)\n",
    "        conv5 = BatchNormalization()(conv5)\n",
    "        drop5 = Dropout(0.5)(conv5, training=True)\n",
    "\n",
    "        up6 = Conv2D(512, 2, activation='relu', padding='same', kernel_initializer='he_normal')(\n",
    "            UpSampling2D(size=(2, 2))(drop5))\n",
    "        merge6 = concatenate([drop4, up6], axis=3)\n",
    "        conv6 = Conv2D(512, 3, activation='relu', padding='same', kernel_initializer='he_normal')(merge6)\n",
    "        conv6 = Conv2D(512, 3, activation='relu', padding='same', kernel_initializer='he_normal')(conv6)\n",
    "\n",
    "        up7 = Conv2D(256, 2, activation='relu', padding='same', kernel_initializer='he_normal')(\n",
    "            UpSampling2D(size=(2, 2))(conv6))\n",
    "        merge7 = concatenate([conv3, up7], axis=3)\n",
    "        conv7 = Conv2D(256, 3, activation='relu', padding='same', kernel_initializer='he_normal')(merge7)\n",
    "        conv7 = Conv2D(256, 3, activation='relu', padding='same', kernel_initializer='he_normal')(conv7)\n",
    "\n",
    "        up8 = Conv2D(128, 2, activation='relu', padding='same', kernel_initializer='he_normal')(\n",
    "            UpSampling2D(size=(2, 2))(conv7))\n",
    "        merge8 = concatenate([conv2, up8], axis=3)\n",
    "        conv8 = Conv2D(128, 3, activation='relu', padding='same', kernel_initializer='he_normal')(merge8)\n",
    "        conv8 = Conv2D(128, 3, activation='relu', padding='same', kernel_initializer='he_normal')(conv8)\n",
    "\n",
    "        up9 = Conv2D(64, 2, activation='relu', padding='same', kernel_initializer='he_normal')(\n",
    "            UpSampling2D(size=(2, 2))(conv8))\n",
    "        merge9 = concatenate([conv1, up9], axis=3)\n",
    "        conv9 = Conv2D(64, 3, activation='relu', padding='same', kernel_initializer='he_normal')(merge9)\n",
    "        conv9 = Conv2D(64, 3, activation='relu', padding='same', kernel_initializer='he_normal')(conv9)\n",
    "        conv9 = Conv2D(2, 3, activation='relu', padding='same', kernel_initializer='he_normal')(conv9)\n",
    "\n",
    "        conv10 = Conv2D(1, 1, activation='sigmoid')(conv9)\n",
    "\n",
    "        return conv10"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def printProgressBar(epoch, iteration, total, prefix='', suffix='', decimals=1, length=100, fill='â–ˆ', printEnd=\"\\r\",\n",
    "                     eta=None, loss=None, train_type='train'):\n",
    "    percent = (\"{0:.\" + str(decimals) + \"f}\").format(100 * (iteration / float(total)))\n",
    "    filledLength = int(length * iteration // total)\n",
    "    bar = fill * filledLength + '-' * (length - filledLength)\n",
    "    print(\n",
    "        f'\\r{prefix} |{bar}| {percent}% {suffix} ETA:{eta} s epoch={epoch}: recon_loss={str(np.round(loss, 4))}',\n",
    "        end=printEnd)\n",
    "    # Print New Line on Complete\n",
    "    if iteration == total:\n",
    "        print()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def generate_and_save_images(model, epoch, test_sample):\n",
    "    logits=model(test_sample)\n",
    "    prediction = logits.numpy()[0] * 255\n",
    "    mask=prediction*255\n",
    "    mask=mask.astype(np.uint8)\n",
    "    mask=skimage.color.gray2rgb(mask)\n",
    "\n",
    "    # plt.imshow(mask[:,:,0,:])\n",
    "    # plt.show()\n",
    "\n",
    "    img=test_sample[0]*255\n",
    "    img=img.astype(np.uint8)\n",
    "    # plt.imshow(img)\n",
    "    # plt.show()\n",
    "    #\n",
    "    # plt.imshow(np.concatenate([mask[:,:,0,:],img],axis=1))\n",
    "    # plt.show()\n",
    "\n",
    "    io.imsave('image_at_epoch_{:04d}.png'.format(epoch), np.concatenate([mask[:,:,0,:],img],axis=1))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test_sample = test_dataset[np.random.randint(0, len(test_dataset))][0][2]\n",
    "generate_and_save_images(model, 0, np.expand_dims(test_sample, axis=0).astype(np.float32))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "images_path = 'data/image/'\n",
    "masks_path = 'data/semantic_label/'\n",
    "\n",
    "images_path_list = glob.glob(images_path + '*')\n",
    "masks_path_list = glob.glob(masks_path + '*')\n",
    "\n",
    "batch_size = 3\n",
    "\n",
    "train_dataset = TextSegDataset(images_path_list, masks_path_list, output_size=512, validation_split=0.2, type='train',batch_size=batch_size)\n",
    "test_dataset = TextSegDataset(images_path_list, masks_path_list, output_size=512, validation_split=0.2, type='test',batch_size=batch_size)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = Unet()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "epochs=100\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
    "beta = 1\n",
    "total_loss=[]\n",
    "\n",
    "l_train=len(train_dataset)\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "\n",
    "\n",
    "    start_time = time.time()\n",
    "    total_loss = []\n",
    "\n",
    "    printProgressBar(epoch, 0, l_train, eta=None, loss=[0, 0, 0], prefix='Progress:', suffix='Complete',\n",
    "                     train_type='train', length=10)\n",
    "    for i,x_train in enumerate(train_dataset):\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            logits=model(x_train[0])\n",
    "\n",
    "            loss=tf.reduce_sum(tf.keras.losses.MSE(logits,x_train[1]))\n",
    "\n",
    "        gradients = tape.gradient(loss, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "        total_loss.append(loss)\n",
    "        printProgressBar(epoch, i + 1, l_train, eta=None, prefix='Progress:', suffix='Complete', train_type='train',\n",
    "                         loss=np.mean(total_loss), length=10)\n",
    "\n",
    "    end_time = time.time()\n",
    "\n",
    "    loss = tf.keras.metrics.Mean()\n",
    "    for i, x_test in enumerate(test_dataset):\n",
    "        logits=model(x_test[0])\n",
    "        test_loss=tf.reduce_sum(tf.keras.losses.MSE(logits,x_test[1]))\n",
    "        loss(test_loss)\n",
    "\n",
    "    print('Epoch: {}, Test set recon: {}, time elapse for current epoch: {}'\n",
    "          .format(epoch, loss.result(), end_time - start_time))\n",
    "\n",
    "    test_sample = test_dataset[np.random.randint(0, len(test_dataset))][0][2]\n",
    "    generate_and_save_images(model, 0, np.expand_dims(test_sample, axis=0).astype(np.float32))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}